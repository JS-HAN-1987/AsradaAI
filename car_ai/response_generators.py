# response_generators.py
import threading
from my_tts import speak, stop_current_speech, force_stop_flag
import time
import re

# ì „ì—­ ì¤‘ë‹¨ í”Œë˜ê·¸ (ëª¨ë“  LLM ì‘ë‹µì—ì„œ ê³µìœ )
STOP_LLM_FLAG = threading.Event()

# ====== ì•ˆì •ì ì¸ ë¬¸ì¥ ì¢…ë£Œ ì •ê·œì‹ ======
# ìˆ«ì(3.14), ë²ˆí˜¸(1.), ..., ì•½ì–´ ë“±ì„ ë°©í•´í•˜ì§€ ì•Šê³ 
# ì§„ì§œ ë¬¸ì¥ ëë§Œ ì¡ëŠ” íŒ¨í„´
SENTENCE_END_REGEX = re.compile(
    r'(?<!\.\.)(?<=[^.0-9])\.(?=\s|$)|[!?](?=\s|$)'
)


def log(msg):
    return
    # print(f"[DEBUG][response_generators][{threading.get_ident()}] {msg}")


# ---------------------------------------------
#  ê³µìš© ë¬¸ì¥ ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------------
def extract_sentences(buffer):
    """
    bufferì—ì„œ ë¬¸ì¥ ë íŒ¨í„´ì„ ì°¾ì•„ sentence ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    ë‚¨ì€ ì”ì—¬ í…ìŠ¤íŠ¸ë„ ë°˜í™˜.
    """
    sentences = []

    while True:
        m = SENTENCE_END_REGEX.search(buffer)
        if not m:
            break

        end = m.end()
        sentence = buffer[:end].strip()
        if sentence:
            sentences.append(sentence)

        buffer = buffer[end:].lstrip()

    return sentences, buffer


def stop_all_llm():
    """ëª¨ë“  LLM ìŠ¤íŠ¸ë¦¬ë°ì„ ì¤‘ë‹¨"""
    STOP_LLM_FLAG.set()
    stop_current_speech()
    time.sleep(0.1)

def reset_llm_stop():
    """LLM ì¤‘ë‹¨ í”Œë˜ê·¸ ì´ˆê¸°í™”"""
    STOP_LLM_FLAG.clear()
    

# ========================================================
#  1) ì°¨ëŸ‰ ë°ì´í„° ì‘ë‹µ
# ========================================================
def generate_car_data_response(llm, question, context):
    log("generate_car_data_response() í˜¸ì¶œë¨")

    if not context:
        return "ì°¨ëŸ‰ ë°ì´í„° ì—†ìŒ."

    prompt = f"""
"ë°ì´í„°" ë¸”ë¡ì—ëŠ” ì°¨ëŸ‰ì˜ ì‹¤ì‹œê°„ ì„¼ì„œ ê°’ì´ í¬í•¨ë˜ì–´ ìˆë‹¤.
"ì§ˆë¬¸"ì— ëŒ€í•´, "ë°ì´í„°" ì•ˆì—ì„œ ë‹µì„ ì°¾ì•„ ì„¤ëª…í•œë‹¤.
ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ì•ŠëŠ”ë‹¤.
ë‹µë³€ì„ ê¸¸ê²Œí•˜ì§€ ì•ŠëŠ”ë‹¤.
ë°ì´í„°: {context}
ì§ˆë¬¸: {question}
"""

    response_text = ""
    last_real_char_time = None

    # ì¤‘ë‹¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
    STOP_LLM_FLAG.clear()

    # ì¤‘ë‹¨ ì²´í¬
    if STOP_LLM_FLAG.is_set():
        return "ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    for chunk in llm.stream(prompt):
        # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
        if STOP_LLM_FLAG.is_set():
            log("ğŸ›‘ LLM ì‘ë‹µ ì¤‘ë‹¨ë¨")
            return "ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        now = time.time()

        if chunk.strip():
            response_text += chunk
            last_real_char_time = now

        # ---- ë¬¸ì¥ ë‹¨ìœ„ ì²˜ë¦¬ (ì •ê·œì‹ ê¸°ë°˜) ----
        sentences, response_text = extract_sentences(response_text)
        for s in sentences:
            # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
            if STOP_LLM_FLAG.is_set() or force_stop_flag.is_set():
                break
            speak(s)

        # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
        if STOP_LLM_FLAG.is_set() or force_stop_flag.is_set():
            break

        # ---- ì¼ì • ì‹œê°„ ë™ì•ˆ new chunk ì—†ìœ¼ë©´ ê°•ì œ flush ----
        if last_real_char_time and (now - last_real_char_time > 1.0) and response_text.strip():
            if not STOP_LLM_FLAG.is_set() and not force_stop_flag.is_set():
                speak(response_text.strip())
            response_text = ""
            last_real_char_time = None

    # ---- ì¢…ë£Œ í›„ ì”ì—¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ----
    if response_text.strip() and not STOP_LLM_FLAG.is_set() and not force_stop_flag.is_set():
        speak(response_text.strip())

    return ""

# ========================================================
#  2) ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
# ========================================================
def generate_general_response(llm, question):
    print(f"[DEBUG][response_generators] generate_general_response() í˜¸ì¶œë¨")

    prompt = f"""
    ì§ˆë¬¸: {question}
    """

    response_text = ""
    last_real_char_time = None

    print(f"[DEBUG][response_generators] LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ ì‹œì‘")

    # ì¤‘ë‹¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
    STOP_LLM_FLAG.clear()

    # ì¤‘ë‹¨ ì²´í¬
    if STOP_LLM_FLAG.is_set():
        return "ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."

    for chunk in llm.stream(prompt):
        # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
        if STOP_LLM_FLAG.is_set():
            log("ğŸ›‘ LLM ì‘ë‹µ ì¤‘ë‹¨ë¨")
            return "ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        now = time.time()

        if chunk.strip():
            response_text += chunk
            last_real_char_time = now

        sentences, response_text = extract_sentences(response_text)
        for s in sentences:
            # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
            if STOP_LLM_FLAG.is_set() or force_stop_flag.is_set():
                break
            speak(s)

        # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
        if STOP_LLM_FLAG.is_set() or force_stop_flag.is_set():
            break

        if last_real_char_time and (now - last_real_char_time > 1.0) and response_text.strip():
            if not STOP_LLM_FLAG.is_set() and not force_stop_flag.is_set():
                speak(response_text.strip())
            response_text = ""
            last_real_char_time = None

    if response_text.strip() and not STOP_LLM_FLAG.is_set() and not force_stop_flag.is_set():
        speak(response_text.strip())


# ========================================================
# 3) ì œì–´ ì‘ë‹µ
# ========================================================
def generate_control_response(question):
    ret = f"'{question}' ëª…ë ¹ì€ í˜„ì¬ ì œì–´ ë¶ˆê°€í•˜ë‹¤."
    return ret
